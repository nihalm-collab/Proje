# ==============================================================================
# PROJE ADI: RAG TabanlÄ± Deprem Bilgilendirme Chatbotu
# AMAÃ‡: Tablosal deprem verilerinden tÃ¼retilen metinleri kullanarak kullanÄ±cÄ±ya
#       bilgi veren bir RAG chatbotu geliÅŸtirmek ve Streamlit ile sunmak.
# ==============================================================================

# ==============================================================================
# BÃ–LÃœM 1: GEREKLÄ° KÃœTÃœPHANELERÄ°N VE ORTAM AYARLARININ Ä°Ã‡E AKTARILMASI
# ==============================================================================
import os
from dotenv import load_dotenv
import pandas as pd # CSV dosyasÄ±nÄ± okumak iÃ§in
import streamlit as st # Web arayÃ¼zÃ¼ iÃ§in

# RAG Mimarisi BileÅŸenleri (LangChain Ã–nerisi)
from langchain_community.document_loaders import DataFrameLoader
from langchain_community.vectorstores import Chroma # VektÃ¶r veritabanÄ±
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings # LLM ve Embedding
from langchain.text_splitter import RecursiveCharacterTextSplitter # Metin parÃ§alama (Chunking)
from langchain.chains import RetrievalQA # RAG Zinciri

# API AnahtarÄ±nÄ± YÃ¼kleme
load_dotenv()
# .env dosyasÄ±ndaki anahtarÄ± ortam deÄŸiÅŸkenlerine yÃ¼kler
# Bu, GEMINI_API_KEY'in doÄŸrudan koda yazÄ±lmasÄ±nÄ± Ã¶nler.
if not os.getenv("GEMINI_API_KEY"):
    st.error("GEMINI_API_KEY, .env dosyasÄ±nda tanÄ±mlanmalÄ±dÄ±r.")

# ==============================================================================
# BÃ–LÃœM 2: VERÄ° HAZIRLAMA VE DÃ–NÃœÅÃœM
# (Knowledge Base OluÅŸturma)
# ==============================================================================

# Sabitler
CHROMA_PATH = "chroma_db_deprem" # VektÃ¶r veritabanÄ±nÄ±n diskte saklanacaÄŸÄ± yer
CSV_FILE = "veriler.csv" # Kaggle'dan indirilen tablosal veri

def load_and_transform_data(csv_file):
    """
    Tablosal veriyi okur ve RAG iÃ§in metinsel formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    try:
        df = pd.read_csv(csv_file)
        
        # DataFrame'deki her bir satÄ±rÄ± (deprem kaydÄ±) metin formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme (KRÄ°TÄ°K ADIM)
        # RAG, tablosal veriden Ã§ok, metinsel bilgilerle daha iyi Ã§alÄ±ÅŸÄ±r.
        # Deprem verilerinden bilgilendirici bir metin tÃ¼retme:
        df['text'] = df.apply(lambda row: (
            f"TÃ¼rkiye'de {row['Olus tarihi']} tarihinde, {row['Yer']} bÃ¶lgesinde "
            f"Moment BÃ¼yÃ¼klÃ¼ÄŸÃ¼ (Mw) {row['Mw']} olarak kaydedilen bir deprem meydana gelmiÅŸtir. "
            f"Depremin odak derinliÄŸi {row['Der (km)']} km'dir. Enlem: {row['Enlem']}, Boylam: {row['Boylam']}. "
            f"Tip: {row['Tip']}."
        ), axis=1)

        # LangChain'in DataFrameLoader'Ä± ile her satÄ±rÄ± bir "Document" nesnesine dÃ¶nÃ¼ÅŸtÃ¼rme
        # Document'in iÃ§eriÄŸi (page_content) 'text' sÃ¼tunundan alÄ±nÄ±r.
        loader = DataFrameLoader(df, page_content_column='text')
        documents = loader.load()
        return documents
    except FileNotFoundError:
        st.error(f"Hata: {csv_file} dosyasÄ± bulunamadÄ±. LÃ¼tfen dosyanÄ±n proje dizininde olduÄŸundan emin olun.")
        return []

# ==============================================================================
# BÃ–LÃœM 3: RAG PIPELINE OLUÅTURMA (Indexing ve Retrieval)
# ==============================================================================

def setup_rag_pipeline(documents):
    """
    Metinleri parÃ§alar (chunk), gÃ¶mer (embed) ve vektÃ¶r veritabanÄ±nÄ± (ChromaDB) kurar.
    """
    if not documents:
        return None, None

    # 3.1. Metin ParÃ§alama (Chunking)
    # RecursiveCharacterTextSplitter, metni ayraÃ§lara gÃ¶re (Ã¶rn. \n\n, \n, boÅŸluk) parÃ§alar.
    # Bu, LLM'in dikkatini kÃ¼Ã§Ã¼k, anlamlÄ± parÃ§alara odaklamasÄ±nÄ± saÄŸlar.
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, # Her parÃ§anÄ±n maksimum karakter boyutu (Ã–rnek DeÄŸer)
        chunk_overlap=200 # ParÃ§alarÄ±n birbiriyle ne kadar Ã¶rtÃ¼ÅŸeceÄŸi (BaÄŸlam korumasÄ± iÃ§in)
    )
    # ParÃ§alanan dokÃ¼manlar
    chunks = text_splitter.split_documents(documents)
    
    # 3.2. GÃ¶mme Modeli (Embedding Model)
    # Metinleri sayÄ±sal vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in kullanÄ±lÄ±r.
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001") # Google'Ä±n Ã¶nerilen embedding modeli
    
    # 3.3. VektÃ¶r VeritabanÄ±na Kaydetme (ChromaDB)
    # VektÃ¶rler, diske kaydedilmek Ã¼zere ChromaDB'ye eklenir. 
    # Bu adÄ±m, her Ã§alÄ±ÅŸtÄ±rmada tekrar yapÄ±lmaz; yalnÄ±zca ilk kurulumda yapÄ±lÄ±r.
    if not os.path.exists(CHROMA_PATH):
        st.info("ChromaDB oluÅŸturuluyor... (Bu iÅŸlem biraz zaman alabilir)")
        db = Chroma.from_documents(
            documents=chunks,
            embedding=embeddings,
            persist_directory=CHROMA_PATH # VeritabanÄ±nÄ± kaydet
        )
        db.persist()
        st.success("ChromaDB baÅŸarÄ±yla oluÅŸturuldu ve kaydedildi.")
    else:
        # Daha Ã¶nce oluÅŸturulmuÅŸ veritabanÄ±nÄ± yÃ¼kle
        db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)
        st.info("Mevcut ChromaDB yÃ¼klendi.")
        
    return db

def create_qa_chain(db):
    """
    Generative Model (LLM) ile Retrieval (Geri Getirme) aracÄ±nÄ± birleÅŸtiren zinciri oluÅŸturur.
    """
    # 3.4. Geri Getirme AracÄ± (Retriever)
    # VektÃ¶r DB'de arama yapar (semantic search).
    retriever = db.as_retriever(search_kwargs={"k": 3}) # En alakalÄ± 3 parÃ§ayÄ± getir
    
    # 3.5. LLM (Generative Model) AyarÄ±
    # ChatGoogleGenerativeAI ile Gemini modelini kullanma
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0.2)
    
    # 3.6. RAG Zinciri (RetrievalQA Chain)
    # Retriever'dan gelen baÄŸlamÄ± LLM'e vererek yanÄ±t Ã¼retme.
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff", # Geri getirilen tÃ¼m parÃ§alarÄ± tek prompt'a ekler
        retriever=retriever,
        return_source_documents=True # CevabÄ±n hangi kaynaktan geldiÄŸini gÃ¶rmek iÃ§in
    )
    
    return qa_chain

# ==============================================================================
# BÃ–LÃœM 4: WEB ARAYÃœZÃœ VE CHATBOT MANTIÄI (STREAMLIT)
# ==============================================================================

def main():
    """
    Streamlit uygulamasÄ±nÄ±n ana fonksiyonu.
    """
    # 4.1. ArayÃ¼z AyarlarÄ± ve Sistem BaÅŸlÄ±ÄŸÄ±
    st.set_page_config(page_title="Deprem Bilgilendirme RAG Chatbotu")
    st.title("TÃ¼rkiye Deprem Bilgilendirme AsistanÄ± ğŸŒ")
    st.caption("RAG (Retrieval Augmented Generation) ile GÃ¼Ã§lendirilmiÅŸtir")
    
    # 4.2. RAG Pipeline Kurulumu
    # Ä°lk Ã§alÄ±ÅŸtÄ±rmada veriyi yÃ¼kle ve RAG zincirini oturum durumuna kaydet
    if "qa_chain" not in st.session_state:
        documents = load_and_transform_data(CSV_FILE)
        if documents:
            db = setup_rag_pipeline(documents)
            st.session_state['qa_chain'] = create_qa_chain(db)
            st.session_state['messages'] = [{"role": "assistant", "content": "Deprem verileri yÃ¼klendi. Hangi deprem hakkÄ±nda bilgi almak istersiniz?"}]
        else:
            return # Veri yoksa uygulamayÄ± sonlandÄ±r

    # 4.3. Chat GeÃ§miÅŸinin GÃ¶sterilmesi
    for message in st.session_state['messages']:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # 4.4. KullanÄ±cÄ±dan Girdi Alma
    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n... (Ã–rn: 6 Åubat 2023 depremi ile ilgili bilgi ver)"):
        
        # KullanÄ±cÄ± mesajÄ±nÄ± geÃ§miÅŸe ekle
        st.session_state['messages'].append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # 4.5. RAG Zincirini Ã‡alÄ±ÅŸtÄ±rma
        with st.spinner("Bilgi aranÄ±yor ve yanÄ±t oluÅŸturuluyor..."):
            qa_chain = st.session_state['qa_chain']
            
            # Zinciri Ã§alÄ±ÅŸtÄ±rma
            response = qa_chain({"query": prompt})

            # RAG YanÄ±tÄ±nÄ± Ä°ÅŸleme
            answer = response["result"]
            
            # Kaynak Belgesini Ekleme (Proje kriteri iÃ§in Ã¶nemlidir)
            source_docs = response["source_documents"]
            sources = "\n".join([doc.page_content for doc in source_docs])
            
            # LLM'e cevabÄ± kaynaklarla birlikte Ã¶zetlemesi iÃ§in System Prompt mantÄ±ÄŸÄ± eklenebilir.
            final_response = f"{answer}\n\n**Kaynaklar (Retrieved Chunks):**\n```\n{sources}\n```"
        
        # 4.6. Asistan CevabÄ±nÄ± GÃ¶sterme
        with st.chat_message("assistant"):
            st.markdown(final_response)
        
        # CevabÄ± geÃ§miÅŸe kaydet
        st.session_state['messages'].append({"role": "assistant", "content": final_response})


if __name__ == '__main__':
    main()
# ==============================================================================
# BÃ–LÃœM 5: KOD ANLATIMININ BÄ°TÄ°ÅÄ°
# TÃ¼m teknik anlatÄ±mlar bu dosya iÃ§erisinde yorum satÄ±rlarÄ± olarak yer almÄ±ÅŸtÄ±r.
# ==============================================================================
