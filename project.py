import streamlit as st
import pandas as pd
import os
from dotenv import load_dotenv

# LangChain/RAG BileÅŸenleri
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document

# Projenin ana veri yolu ve konfigÃ¼rasyonlarÄ±
# NOT: Kaggle'da 'os.getenv' ile secrets/ortam deÄŸiÅŸkenlerini Ã§ekebilirsiniz.
# Lokal Ã§alÄ±ÅŸtÄ±rma iÃ§in .env dosyasÄ± kullanÄ±labilir.
load_dotenv()
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# =========================================================================
# Veri HazÄ±rlama FonksiyonlarÄ±
# =========================================================================

@st.cache_resource
def load_and_prepare_data():
    """
    Kaggle CSV dosyasÄ±nÄ± yÃ¼kler ve her satÄ±rÄ± RAG iÃ§in metin belgesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    Bu, tabular veriyi metin tabanlÄ± RAG iÃ§in uygun hale getirmenin kritik adÄ±mÄ±dÄ±r.
    """
    try:
        # Kaggle'da veri setine eriÅŸim yolu bu formattadÄ±r.
        # KullanÄ±cÄ±nÄ±n veri seti yolunu doÄŸru girdiÄŸinden emin olun.
        data_path = "/kaggle/input/turkey-earthquake-data-1914-2023/earthquake.csv"
        df = pd.read_csv(data_path)
    except FileNotFoundError:
        st.error("Veri seti bulunamadÄ±. LÃ¼tfen Kaggle ortamÄ±nda doÄŸru yolu kullandÄ±ÄŸÄ±nÄ±zdan emin olun.")
        return []

    documents = []
    for index, row in df.iterrows():
        # Her bir deprem kaydÄ±nÄ± anlamlÄ±, tek bir metin parÃ§asÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme
        text_content = (
            f"Tarih: {row['date_']}. Enlem: {row['latitude']:.2f}, Boylam: {row['longitude']:.2f}. "
            f"Åiddet (Magnitude): {row['magnitude']:.1f} ({row['type']}). "
            f"Derinlik: {row['depth']:.1f} km. BÃ¶lge: {row['region_']}"
        )
        documents.append(Document(page_content=text_content, metadata={"source": row['date_'], "magnitude": row['magnitude']}))

    st.success(f"{len(documents)} adet deprem kaydÄ± metin belgesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼ldÃ¼.")
    return documents

# =========================================================================
# RAG Pipeline Kurulumu
# =========================================================================

@st.cache_resource
def setup_rag_pipeline(documents):
    """
    RAG mimarisini kurar: Ã‡anklama, Embedding, VektÃ¶r DB ve QA Zinciri.
    """
    if not documents:
        return None

    # 1. Metin ParÃ§alama (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=100,
        length_function=len
    )
    texts = text_splitter.split_documents(documents)
    st.info(f"Metinler {len(texts)} parÃ§aya (chunk) bÃ¶lÃ¼ndÃ¼.")

    # 2. Embedding Modeli ve VektÃ¶r DB (ChromaDB)
    embeddings = GoogleGenerativeAIEmbeddings(model="text-embedding-004")
    db = Chroma.from_documents(texts, embeddings, persist_directory="./chroma_db")
    
    # 3. Retriever: En alakalÄ± 3 belgeyi geri Ã§aÄŸÄ±rÄ±r
    retriever = db.as_retriever(search_kwargs={"k": 3})

    # 4. Prompt Åablonu: LLM'i sadece saÄŸlanan baÄŸlama gÃ¶re cevap vermeye zorlar
    template = """Sen TÃ¼rkiye'deki deprem verileri hakkÄ±nda uzman bir bilgi asistanÄ±sÄ±n.
    AÅŸaÄŸÄ±daki baÄŸlamÄ± (CONTEXT) kullanarak kullanÄ±cÄ±nÄ±n sorusunu (QUESTION) cevapla.
    Sadece baÄŸlamda bulunan bilgilere dayan. EÄŸer baÄŸlamda cevap yoksa,
    'Bu soruya elimdeki verilerle cevap veremiyorum.' ÅŸeklinde yanÄ±tla.
    CevabÄ±n kapsamlÄ± ve anlaÅŸÄ±lÄ±r olsun.
    
    CONTEXT: {context}
    QUESTION: {question}
    ANSWER:
    """
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])

    # 5. RetrievalQA Zinciri
    qa_chain = RetrievalQA.from_chain_type(
        llm=ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.2), # DÃ¼ÅŸÃ¼k temperature halÃ¼sinasyonu azaltÄ±r
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}
    )
    st.success("RAG Pipeline baÅŸarÄ±yla kuruldu.")
    return qa_chain

# =========================================================================
# Streamlit ArayÃ¼zÃ¼
# =========================================================================

def main():
    """
    Streamlit arayÃ¼zÃ¼nÃ¼ oluÅŸturur ve chatbot mantÄ±ÄŸÄ±nÄ± yÃ¶netir.
    """
    st.title("ğŸ‡¹ğŸ‡· Deprem Verileri Bilgi AsistanÄ± (RAG Chatbot)")
    st.caption("Akbank GenAI Bootcamp Projesi | Veri: 1914-2023 TÃ¼rkiye Deprem Verileri")

    if not GEMINI_API_KEY:
        st.warning("LÃ¼tfen `GEMINI_API_KEY` ortam deÄŸiÅŸkenini/Kaggle Secret'Ä± ayarlayÄ±n.")
        return

    # Veri yÃ¼kleme ve RAG pipeline kurulumu
    with st.spinner("Veriler yÃ¼kleniyor ve RAG mimarisi kuruluyor..."):
        documents = load_and_prepare_data()
        qa_chain = setup_rag_pipeline(documents)

    if qa_chain is None:
        st.error("RAG kurulumu baÅŸarÄ±sÄ±z oldu.")
        return

    # Sohbet geÃ§miÅŸini saklama
    if "messages" not in st.session_state:
        st.session_state["messages"] = [
            {"role": "assistant", "content": "Deprem verileri hakkÄ±nda ne Ã¶ÄŸrenmek istersiniz? Ã–rneÄŸin: 'En bÃ¼yÃ¼k 5 deprem ne zaman oldu?'"}
        ]

    # Sohbet geÃ§miÅŸini gÃ¶sterme
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # KullanÄ±cÄ± giriÅŸi
    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        st.chat_message("user").write(prompt)

        # RAG sorgusunu Ã§alÄ±ÅŸtÄ±r
        with st.spinner("Cevap aranÄ±yor..."):
            result = qa_chain({"query": prompt})
            
            # Cevap ve KaynaklarÄ± biÃ§imlendir
            response = result["result"]
            sources = result["source_documents"]
            
            # KaynaklarÄ± gÃ¶ster
            source_info = "## Kaynaklar:\n"
            for i, doc in enumerate(sources):
                source_info += f"- **{i+1}. Kaynak (Åiddet: {doc.metadata.get('magnitude'):.1f})**: {doc.page_content}\n"

            assistant_response = f"{response}\n\n---\n\n{source_info}"

        st.session_state.messages.append({"role": "assistant", "content": assistant_response})
        st.chat_message("assistant").write(assistant_response)

if __name__ == "__main__":
    main()
