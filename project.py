import os
import pandas as pd
import streamlit as st
from dotenv import load_dotenv

# LangChain bileÅŸenleri
from langchain_community.document_loaders import DataFrameLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings

# -----------------------------------------------------------
# 1. ORTAM AYARLARI VE SABÄ°T TANIMLAMALAR
# -----------------------------------------------------------

# .env dosyasÄ±nÄ± yÃ¼kle (Lokalde Ã§alÄ±ÅŸÄ±rken API anahtarÄ±nÄ± alÄ±r)
load_dotenv()

# API AnahtarÄ±nÄ± kontrol et ve ayarla (Kaggle/Colab'de zaten ortam deÄŸiÅŸkeni olarak bulunur)
if "GEMINI_API_KEY" not in os.environ:
    st.error("LÃ¼tfen GEMINI_API_KEY ortam deÄŸiÅŸkenini ayarlayÄ±n (Lokalde .env, Kaggle'da Secrets).")
    st.stop()

# Model ve Veri TabanÄ± Sabitleri
VECTOR_DB_DIR = "chroma_db"
LLM_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL_NAME = "models/embedding-001" # Google'Ä±n Ã¶nerdiÄŸi Embedding modeli

# -----------------------------------------------------------
# 2. VERÄ° HAZIRLIÄI VE METÄ°NLEÅTÄ°RME FONKSÄ°YONLARI
# -----------------------------------------------------------

def load_data_and_create_documents():
    """
    Kaggle CSV dosyasÄ±nÄ± yÃ¼kler, her bir satÄ±rÄ± RAG iÃ§in uygun metin belgesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    """
    try:
        # Kaggle veri setinin yolu (Ortama gÃ¶re yolu gÃ¼ncelleyebilirsiniz)
        data_path = "earthquake_data_1914_2023.csv"
        
        # Veri setini pandas ile oku
        df = pd.read_csv(data_path)
        
        # Sadece kritik sÃ¼tunlarÄ± al
        df = df[['Time', 'Latitude', 'Longitude', 'Magnitude', 'Depth/Km', 'Region', 'City']]
        
        # Her bir satÄ±rÄ± metin belgesine dÃ¶nÃ¼ÅŸtÃ¼rme fonksiyonu
        def create_document_content(row):
            return (
                f"Tarih ve Saat: {row['Time']}. BÃ¶lge/Åehir: {row['Region']} / {row['City']}. "
                f"BÃ¼yÃ¼klÃ¼k: {row['Magnitude']} ÅŸiddetinde. Derinlik: {row['Depth/Km']} km. "
                f"Koordinatlar: {row['Latitude']} enlem, {row['Longitude']} boylam."
            )

        # TÃ¼m DataFrame'i LangChain Document nesnelerine dÃ¶nÃ¼ÅŸtÃ¼r (MetinleÅŸtirme)
        # Her bir satÄ±r bir LangChain Document'Ä± olacaktÄ±r.
        documents = []
        for index, row in df.iterrows():
             documents.append({
                 "page_content": create_document_content(row),
                 "metadata": {"source": f"KayÄ±t {index+1}"}
             })

        # LangChain'in DataFrameLoader'Ä± metin iÃ§eriÄŸini `page_content` anahtarÄ± ile bekler.
        # Bu yaklaÅŸÄ±m, dokÃ¼man metadata'sÄ±nÄ± daha iyi kontrol etmemizi saÄŸlar.
        # DataFrameLoader yerine manuel listeyi kullanÄ±yoruz:
        docs = [
            {"page_content": create_document_content(row), "metadata": {"source": f"KayÄ±t {index+1}", "Region": row['Region'], "Magnitude": row['Magnitude']}}
            for index, row in df.iterrows()
        ]
        
        # DokÃ¼man iÃ§eriÄŸi doÄŸru bir ÅŸekilde LangChain Document objesine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor
        from langchain.schema import Document
        documents = [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in docs]

        return documents

    except FileNotFoundError:
        st.error(f"Veri dosyasÄ± bulunamadÄ±: {data_path}. Kaggle ortamÄ±nda veya yerelde dosyanÄ±n adÄ±nÄ± kontrol edin.")
        return []
    except Exception as e:
        st.error(f"Veri yÃ¼kleme veya dÃ¶nÃ¼ÅŸtÃ¼rme hatasÄ±: {e}")
        return []

# -----------------------------------------------------------
# 3. RAG PIPELINE KURULUMU VE Ä°NDEKLEME
# -----------------------------------------------------------

@st.cache_resource
def initialize_rag_pipeline():
    """
    RAG zincirini (Embedding, Vector Store, Retriever, LLM) kurar.
    Streamlit'in cache_resource dekoratÃ¶rÃ¼ ile veritabanÄ±nÄ±n sadece bir kez oluÅŸturulmasÄ± saÄŸlanÄ±r.
    """
    st.write("RAG Sistemi BaÅŸlatÄ±lÄ±yor: Veri YÃ¼kleme ve Ä°ndeksleme...")

    # 1. Veri YÃ¼kleme ve MetinleÅŸtirme
    documents = load_data_and_create_documents()
    if not documents:
        return None

    # 2. ParÃ§alama (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=100, 
        separators=["\n\n", "\n", " ", ""]
    )
    splits = text_splitter.split_documents(documents)
    
    # 3. Embedding Model
    # Google'Ä±n Embedding modelini kullan
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL_NAME)
    
    # 4. VektÃ¶r Depolama (ChromaDB) ve Ä°ndeksleme
    # ChromaDB'yi yerel diskte (veya Colab/Kaggle'da oturum belleÄŸinde) oluÅŸtur
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=embeddings, 
        persist_directory=VECTOR_DB_DIR
    )
    
    # 5. Retriever (Geri Ã‡aÄŸÄ±rma)
    # En alakalÄ± 3 dokÃ¼manÄ± Ã§ekecek ÅŸekilde ayarla
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    
    # 6. LLM (Generation Model)
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0.1) # DÃ¼ÅŸÃ¼k sÄ±caklÄ±k ile veriye sadÄ±k kalma
    
    # 7. Prompt MÃ¼hendisliÄŸi: HalÃ¼sinasyonu Ã¶nlemek iÃ§in kritik prompt
    system_prompt = (
        "Sen, TÃ¼rkiye Deprem Verileri (1914-2023) hakkÄ±nda uzmanlaÅŸmÄ±ÅŸ bir bilgi asistanÄ±sÄ±n. "
        "LÃ¼tfen **SADECE** saÄŸlanan baÄŸlam (context) iÃ§inde yer alan deprem kayÄ±tlarÄ±na gÃ¶re cevap ver. "
        "EÄŸer bilgi baÄŸlamda yoksa, 'Bu bilgi veri setinde bulunmamaktadÄ±r.' diye cevapla. "
        "CevaplarÄ±nÄ± dÃ¼zenli ve bilgilendirici bir dille ver. "
        "\n\nContext: {context}"
    )
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])
    
    # 8. RAG Zinciri OluÅŸturma (LangChain)
    document_chain = create_stuff_documents_chain(llm, prompt)
    retrieval_chain = create_retrieval_chain(retriever, document_chain)
    
    st.write(f"RAG Sistemi BaÅŸlatÄ±ldÄ±! {len(splits)} adet metin parÃ§asÄ± indekslendi.")
    return retrieval_chain

# -----------------------------------------------------------
# 4. STREAMLIT WEB ARAYÃœZÃœ (app.py'nin Ana Fonksiyonu)
# -----------------------------------------------------------

def main():
    """Streamlit uygulamasÄ±nÄ± Ã§alÄ±ÅŸtÄ±ran ana fonksiyon."""
    st.title("ğŸ‡¹ğŸ‡· Deprem Verileri Bilgi AsistanÄ± (RAG)")
    
    # RAG Pipeline'Ä± baÅŸlat ve Ã¶nbelleÄŸe al
    rag_chain = initialize_rag_pipeline()

    if rag_chain is None:
        st.stop()

    # Sohbet geÃ§miÅŸini baÅŸlat (Streamlit Session State)
    if "messages" not in st.session_state:
        st.session_state.messages = []
        
    # BaÅŸlangÄ±Ã§ mesajÄ±nÄ± ekle
    if not st.session_state.messages:
        st.session_state.messages.append({"role": "assistant", "content": "Merhaba! TÃ¼rkiye Deprem Verileri (1914-2023) hakkÄ±nda dilediÄŸiniz soruyu sorabilirsiniz. Veri setine sadÄ±k kalarak cevap vereceÄŸim."})

    # Sohbet geÃ§miÅŸini gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Sorunuzu buraya yazÄ±n (Ã–rn: En bÃ¼yÃ¼k deprem ne zaman oldu?)"):
        # KullanÄ±cÄ± mesajÄ±nÄ± ekle
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Chatbot yanÄ±tÄ±nÄ± Ã¼ret
        with st.chat_message("assistant"):
            with st.spinner("YanÄ±t aranÄ±yor..."):
                try:
                    # RAG zincirini Ã§aÄŸÄ±r
                    response = rag_chain.invoke({"input": prompt})
                    
                    # CevabÄ± al
                    answer = response["answer"]
                    st.markdown(answer)
                    
                    # KullanÄ±lan kaynaklarÄ± (opsiyonel) gÃ¶ster
                    sources = [doc.metadata.get("source", "Bilinmeyen Kaynak") for doc in response["context"]]
                    if sources:
                        unique_sources = list(set(sources))
                        st.caption(f"KullanÄ±lan Kaynaklar: {', '.join(unique_sources)}")
                        
                    st.session_state.messages.append({"role": "assistant", "content": answer})

                except Exception as e:
                    error_message = f"Bir hata oluÅŸtu: {e}. LÃ¼tfen API anahtarÄ±nÄ±zÄ± ve baÄŸlantÄ±yÄ± kontrol edin."
                    st.error(error_message)
                    st.session_state.messages.append({"role": "assistant", "content": error_message})

if __name__ == "__main__":
    main()
